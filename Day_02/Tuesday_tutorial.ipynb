{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring MIRI data: PAHs and dust in the ISM of nearby galaxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1) Import all packages for general usage in this notebook (specific libraries will be considered separately)\n",
    "2) Define path to __YOUR__ directory where all the data for this tutorial (originals and processed) will be stored\n",
    "3) Copy all files from the _read-only_ source directory to the folder that you just defined (this will create new directory, if it does not exist)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=== How to install the kernel for Tuesday session ===\n",
    "\n",
    "We start by setting up the environment - please copy this command in the Terminal:\n",
    "\n",
    "conda activate /media/home/team_workspaces/JWST-Heidelberg-Summer-School/software/pahs/media/home/team_workspaces/JWST-Heidelberg-Summer-School/software/pahs/bin/python3.10 -m ipykernel install --user  --name 'pahs_tuesday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all packages for general usage\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.wcs import WCS\n",
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.visualization import ImageNormalize, PercentileInterval, AsinhStretch, LinearStretch\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "pypher_cmd = \"/media/home/team_workspaces/JWST-Heidelberg-Summer-School/software/pahs/bin/pypher\"\n",
    "addpixscl_cmd = \"/media/home/team_workspaces/JWST-Heidelberg-Summer-School/software/pahs/bin/addpixscl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work directory !! Change to your folder in the Student_Area\n",
    "work_dir = \"/media/home/team_workspaces/JWST-Heidelberg-Summer-School/Students_Area/yuxingzn/tuesday_session\"\n",
    "\n",
    "# === Don't change these lines. They define the folder structure that is used in this tutorial.\n",
    "# Also, try to use these defined variables in the excercises\n",
    "psf_dir = os.path.join(work_dir, 'PSF')  # folder with PSFs\n",
    "kernels_dir = os.path.join(work_dir, 'PSF', 'kernels') # subfolder with convolution kernels\n",
    "jwst_data_dir = os.path.join(work_dir, 'PHANGS-JWST') # folder with JWST images\n",
    "jwst_bgrcorr_data_dir = os.path.join(jwst_data_dir, 'bgrcorr') # folder with JWST images after background matching\n",
    "convolved_data_dir = os.path.join(jwst_data_dir, 'convolved') # folder with convolved JWST images to match resolution of other data\n",
    "convolved_bgrcorr_data_dir = os.path.join(jwst_data_dir, 'convolved_bgrcorr') # folder with convolved and background-mactched JWST images (use this for science analysis)\n",
    "muse_data_dir = os.path.join(work_dir, 'PHANGS-MUSE') # folder with MUSE Halpha images\n",
    "spitzer_data_dir = os.path.join(work_dir, 'Spitzer') # folder with Spitzer images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start copying/linking files\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# copy/link files to your work directory\n",
    "if not os.path.isdir(work_dir):\n",
    "    print(\"Something wrong with the path to your work_dir. Check it\")\n",
    "else:\n",
    "    print(\"Start copying/linking files\")\n",
    "    os.chdir(work_dir)\n",
    "    for d in [\"PHANGS-JWST/convolved\", \"PHANGS-JWST/bgrcorr\", \"PHANGS-JWST/astrometry_align_demo\", \"PHANGS-JWST/convolved_bgrcorr\", \n",
    "                \"PHANGS-MUSE\", \"Spitzer\", \"PSF/gauss\", \"PSF/jwst\", \"PSF/kernels\", \"PSF/spitzer\"]:\n",
    "        if not os.path.isdir(d):\n",
    "            os.makedirs(d)\n",
    "    source_dir = \"/media/home/team_workspaces/JWST-Heidelberg-Summer-School/Lecturers_Area/06_Tuesday_Session_3/data/\"\n",
    "    files = glob.glob(source_dir+\"PHANGS-JWST/*.fits\")\n",
    "    for f in files:\n",
    "        f_target = f\"PHANGS-JWST/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            os.symlink(f, f_target)\n",
    "    files = glob.glob(source_dir+\"PHANGS-JWST/bgrcorr/*.fits\")\n",
    "    for f in files:\n",
    "        f_target=f\"PHANGS-JWST/bgrcorr/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            os.symlink(f, f_target)\n",
    "    files = glob.glob(source_dir+\"PHANGS-JWST/astrometry_align_demo/*.fits\")\n",
    "    for f in files:\n",
    "        f_target=f\"PHANGS-JWST/astrometry_align_demo/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            os.symlink(f, f_target)\n",
    "    files = glob.glob(source_dir+\"PHANGS-MUSE/*.fits\")\n",
    "    for f in files:\n",
    "        f_target=f\"PHANGS-MUSE/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            os.symlink(f, f_target)\n",
    "    files = glob.glob(source_dir+\"Spitzer/*.fits\")\n",
    "    for f in files:\n",
    "        f_target=f\"Spitzer/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            os.symlink(f, f_target)\n",
    "            \n",
    "    files = glob.glob(source_dir+\"PHANGS-JWST/convolved_bgrcorr/*.fits\")\n",
    "    for f in files:\n",
    "        f_target=f\"PHANGS-JWST/convolved_bgrcorr/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            shutil.copy(f, f_target)\n",
    "    \n",
    "    files = glob.glob(source_dir+\"PHANGS-JWST/convolved/*.fits\")\n",
    "    for f in files:\n",
    "        f_target=f\"PHANGS-JWST/convolved/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            shutil.copy(f, f_target)\n",
    "    \n",
    "    files = glob.glob(source_dir+\"PSF/gauss/*.fits\")\n",
    "    for f in files:\n",
    "        f_target=f\"PSF/gauss/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            shutil.copy(f, f_target)\n",
    "    \n",
    "    files = glob.glob(source_dir+\"PSF/jwst/*.fits\")\n",
    "    for f in files:\n",
    "        f_target=f\"PSF/jwst/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            shutil.copy(f, f_target)\n",
    "    \n",
    "    files = glob.glob(source_dir+\"PSF/kernels/*.fits\")\n",
    "    for f in files:\n",
    "        f_target=f\"PSF/kernels/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            shutil.copy(f, f_target)\n",
    "    \n",
    "    files = glob.glob(source_dir+\"PSF/spitzer/*.fits\")\n",
    "    for f in files:\n",
    "        f_target=f\"PSF/spitzer/{f.split('/')[-1]}\"\n",
    "        if not os.path.exists(f_target):\n",
    "            os.symlink(f, f_target)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Quick look at the data\n",
    "\n",
    "Let's look at the JWST/MIRI F770W and F2100W intensities and try to plot $R_{PAH} = F770W/F2100W$. The latter ratio is a good tracer of PAH molecules mass fraction in the dust component of ISM.\n",
    "(note that $R_{PAH} = F770W+F1130W/F2100W$ is often prefered, but here we neglect F1130W for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the name of the galaxy that you'd like to see\n",
    "# options: ngc628, ngc1566, ngc4254, ic5332\n",
    "galaxy = 'ngc4254'\n",
    "\n",
    "# Go to the folder with JWST data\n",
    "os.chdir(jwst_data_dir)\n",
    "\n",
    "# Read F770W and F2100W images (from 'SCI' extension = 1) and calculate R_PAH = F770W/F2100W\n",
    "f770w = fits.getdata(f\"{galaxy}_f770w.fits\", 1)\n",
    "f2100w = fits.getdata(f\"{galaxy}_f2100w.fits\", 1)\n",
    "\n",
    "rpah = f770w/f2100w\n",
    "\n",
    "# phantom void\n",
    "# xslice = slice(250,700)\n",
    "# yslice = slice(550,1000)\n",
    "\n",
    "\n",
    "# Select only part of the images to show (in pixels)\n",
    "xslice = slice(1100,1540)\n",
    "yslice = slice(660,1100)\n",
    "\n",
    "\n",
    "# Show images\n",
    "fig = plt.figure(figsize=(21,6))\n",
    "fig.add_subplot(131)\n",
    "norm = ImageNormalize(f770w, stretch=LinearStretch(), interval=PercentileInterval(98.)) \n",
    "plt.imshow(f770w[yslice, xslice], origin='lower', interpolation='nearest', norm=norm, cmap=plt.cm.cubehelix)\n",
    "plt.title(f\"{galaxy}, MIRI F770W\")\n",
    "plt.colorbar(label='MJy/sr')\n",
    "\n",
    "fig.add_subplot(132)\n",
    "norm = ImageNormalize(f2100w, stretch=LinearStretch(), interval=PercentileInterval(98.))\n",
    "plt.imshow(f2100w[yslice, xslice], origin='lower', interpolation='nearest', norm=norm, cmap=plt.cm.cubehelix)\n",
    "plt.title(f\"{galaxy}, MIRI F2100W\")\n",
    "plt.colorbar(label='MJy/sr')\n",
    "\n",
    "fig.add_subplot(133)\n",
    "plt.imshow(rpah[yslice, xslice], origin='lower', interpolation='nearest', vmin=0.5, vmax=3, cmap=plt.cm.cubehelix)\n",
    "plt.title(f\"{galaxy.upper()}, $R_{{PAH}}$ = F770W/F2100W (unmatched)\")\n",
    "plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can erroneously interpret the thin filamentary structures of high $R_{PAH}$ as a signature of elevated PAH mass fraction there. This is wrong because we haven't performed yet one important step - PSF matching.\n",
    "Resolution of these images in F770W and F2100W differs significantly (XXX for F770W and 0.67 for F2100W), therefore F2100W appears as smoother image (outside prominent clumps), and the regions of high $R_{PAH}$ just trace better resolved structure of ISM visible in F770W.\n",
    "\n",
    "__PSF (beam) matching is critical for proper comparison of any data sets obtained with different instruments/in different bands__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. PSF matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to match two images with different resolution/PSF, we need to measure the PSF itself for each of these images. After that, we can compute kernel that can be used for convolution of \"image1\" to match \"image2\". \n",
    "How to construct kernels is described in detail in <a href=\"https://ui.adsabs.harvard.edu/abs/2011PASP..123.1218A\">Aniano et al. (2011)</a>. They also produced numerous kernels for convolution between various instruments, and IDL code to implement all their recepies. In python, you can use e.g. <a href=\"https://photutils.readthedocs.io/en/stable/\">photutils</a> or <a href=\"https://github.com/aboucaud/pypher\">pypher</a> (described in <a href=\"https://ui.adsabs.harvard.edu/abs/2016A%26A...596A..63B\">Boucaud et al. (2016)</a>). Here we will use pypher with additional bespoke pre- and post-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build JWST PSFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://webbpsf.readthedocs.io/en/latest/intro.html'>WebbPSF</a> tool (<a href=\"https://ui.adsabs.harvard.edu/abs/2014SPIE.9143E..3XP\">Perrin et al. 2014</a>) can simulate PSF of any JWST instrument. In particular, in the latest version for today (1.2.1), it is possible to create PSF that will match the metadata in the observations (e.g. instrument, fiter, date etc.)\n",
    "\n",
    "Let's generate oversampled PSFs for F770W and F2100W filters for any two galaxies in our sample and compare them.\n",
    "We will need oversampled PSFs later for generate kernels with a better precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WEBBPSF_PATH'] = '/media/home/team_workspaces/JWST-Heidelberg-Summer-School/software/webbpsf/webbpsf-data/'\n",
    "import webbpsf\n",
    "\n",
    "# instrument = webbpsf.MIRI()\n",
    "# instrument.filter = 'F770W'\n",
    "\n",
    "galaxies = ['ngc628', 'ngc1566']\n",
    "\n",
    "psf_f770w = []\n",
    "psf_f2100w = []\n",
    "\n",
    "for gal in galaxies:\n",
    "    miri_f770 = webbpsf.setup_sim_to_match_file(os.path.join(jwst_data_dir, f'{gal}_f770w.fits'))\n",
    "    miri_f2100 = webbpsf.setup_sim_to_match_file(os.path.join(jwst_data_dir, f'{gal}_f2100w.fits'))\n",
    "    sim_psf = miri_f770.calc_psf(oversample=4, fov_arcsec=20)\n",
    "    psf_f770w.append(sim_psf)\n",
    "\n",
    "    sim_psf = miri_f2100.calc_psf(oversample=4, fov_arcsec=20)\n",
    "    psf_f2100w.append(sim_psf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PSFs for F2100W\n",
    "fig, ax = plt.subplots(1,3,figsize=(18,5))\n",
    "for ind in range(2):\n",
    "    plt.sca(ax[ind])\n",
    "    webbpsf.display_psf(psf_f2100w[ind], ext='DET_DIST', vmax=0.1, vmin=1e-6, title=f'MIRI/F2100W PSF for {galaxies[ind]}')\n",
    "plt.sca(ax[-1])\n",
    "difference = psf_f2100w[1]['DET_DIST'].data-psf_f2100w[0]['DET_DIST'].data\n",
    "norm = ImageNormalize(difference, interval=PercentileInterval(97))\n",
    "plt.imshow(difference, norm=norm, origin='lower', interpolation='nearest')\n",
    "plt.title('Difference')\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot PSFs for F770W\n",
    "fig, ax = plt.subplots(1,3,figsize=(18,5))\n",
    "for ind in range(2):\n",
    "    plt.sca(ax[ind])\n",
    "    webbpsf.display_psf(psf_f770w[ind], ext='DET_DIST', vmax=0.1, vmin=1e-6, title=f'MIRI/F770W PSF for {galaxies[ind]}')\n",
    "plt.sca(ax[-1])\n",
    "difference = psf_f770w[1]['DET_DIST'].data-psf_f770w[0]['DET_DIST'].data\n",
    "norm = ImageNormalize(difference, interval=PercentileInterval(98))\n",
    "plt.imshow(difference, norm=norm, origin='lower', interpolation='nearest')\n",
    "plt.title('Difference')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As follows from the comparison, there are slight differences between PSFs for objects observed in defferent days. However, the absolute level of these differences is very low. Therefore, for simplicity, we will use further same PSFs for all galaxies. However, for the proper analysis, it might be better to consider PSFs best matching your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save 'overdist' extension corresponding to 'oversampled PSF with detectro distorsions\n",
    "fits.writeto(os.path.join(psf_dir, 'jwst', 'psf_miri_f2100w.fits'), data=psf_f2100w[0]['OVERDIST'].data, header=psf_f2100w[0]['OVERDIST'].header, overwrite=True)\n",
    "fits.writeto(os.path.join(psf_dir, 'jwst', 'psf_miri_f770w.fits'), data=psf_f770w[0]['OVERDIST'].data, header=psf_f770w[0]['OVERDIST'].header, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circularize PSFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JWST PSFs (as well as of many other instruments) do not have azimuthal symmetry. In observations of the extended sources, its PSF angle can be not uniform across the FOV, and especially when several pointings obtained at different times are mosaicked together during the data reduction. Therefore, it is useful to consider azymuthaly symmetrical \"circularized\" PSFs for building convolution kernels.\n",
    "\n",
    "Here we create and apply function that circularizes PSF.\n",
    "To do this, we will construct new PSF summing the 2^14 times rotated PSF with 360/2^14 degree step. This can be preformed in just 14 iterations following the recepie from <a href=\"https://ui.adsabs.harvard.edu/abs/2011PASP..123.1218A\">Aniano et al. (2011)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate\n",
    "def circularize(original_psf, n_rotations=14):\n",
    "    data = original_psf.copy()\n",
    "    for n in range(n_rotations, 0, -1):\n",
    "        psf_rotate = rotate(data, 360 / (2 ** n), order=3, reshape=False)\n",
    "        data = 0.5 * (data + psf_rotate)\n",
    "\n",
    "    radius = np.min(data.shape) / 2\n",
    "    x_cen = (data.shape[0] - 1) / 2\n",
    "    y_cen = (data.shape[1] - 1) / 2\n",
    "\n",
    "    xx, yy = np.meshgrid((np.arange(data.shape[1]) - x_cen),\n",
    "                         (np.arange(data.shape[0]) - y_cen), indexing='xy')\n",
    "\n",
    "    data[(xx ** 2 + yy ** 2) > radius ** 2] = 0\n",
    "\n",
    "    # Round anything within machine uncertainty to 0\n",
    "    data[np.abs(data) < np.finfo(float).eps] = 0\n",
    "    data = data/np.nansum(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply circularization to the original JWST PSFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_files = glob.glob(os.path.join(psf_dir,'jwst','*w.fits'))\n",
    "for file in psf_files:\n",
    "    file_out = file.split('.fits')[0]+'_circ.fits'\n",
    "    with fits.open(file) as hdu:\n",
    "        psf_circularized = circularize(hdu[0].data)\n",
    "        hdu[0].data = psf_circularized\n",
    "        hdu.writeto(file_out, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all PSFs in the directory, exclude circularized (_circ)\n",
    "psf_files_circ= glob.glob(os.path.join(psf_dir,'jwst','*circ.fits'))\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "for ind, f in enumerate(psf_files_circ):\n",
    "    filter = re.search('(f\\d+w)',f).group().upper()\n",
    "    ax = fig.add_subplot(100+len(psf_files_circ)*10+ind+1)\n",
    "    with fits.open(f) as hdu:\n",
    "        norm = ImageNormalize(hdu[0].data, interval=PercentileInterval(99.))\n",
    "        plt.imshow(hdu[0].data, origin='lower', interpolation='nearest', norm=norm)\n",
    "        plt.title(filter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create kernels with pypher\n",
    "\n",
    "Now, let's create circularized kernel for F770W -> F2100W convolution. We will use pypher for that. \n",
    "Note that it requires pixel scale to be defined in wcs of the PSF. However, webbpsf doesn't create it. Therefore, we need to run first \"addpixscl\" from pypher in order to manually add this info to the fits header of all PSFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(psf_dir)\n",
    "# Delete the kernel if already exists. \n",
    "file_kernel = os.path.join(kernels_dir, 'F770W_to_F2100W_circ.fits')\n",
    "if os.path.isfile(file_kernel):\n",
    "    os.remove(file_kernel)\n",
    "\n",
    "# modify PSF file: add pxscale to wcs\n",
    "pxscale = fits.getheader('jwst/psf_miri_f770w_circ.fits')['PIXELSCL']\n",
    "os.system(f\"{addpixscl_cmd} jwst/psf_miri_f770w_circ.fits jwst/psf_miri_f2100w_circ.fits jwst/psf_miri_f1130w_circ.fits {pxscale}\")\n",
    "\n",
    "#pypher team recomends to use regularization parameter (-r) about 1/(S/N) for your source. \n",
    "os.system(f\"{pypher_cmd} jwst/psf_miri_f770w_circ.fits jwst/psf_miri_f2100w_circ.fits {file_kernel} -r 1e-2\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function to postprocess our kernel:\n",
    " 1) Trim kernel at a given total energy fraction threshold. By default, we are going to make kernels that redistribute 99.9% of the total flux and slightly reduce the sizes of the kernels. This can help to speed up the convolution and potentially minimize number of artifacts due to extended uncertain wings\n",
    " 2) Resample kernel to the original detector pixel scale\n",
    " 3) Adjust its shape, if necessary, to make sure it has odd number of pixels along each axis\n",
    " 4) Normalize kernel -> integral must be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "\n",
    "def postprocess_kernel(file_kernel, fileout=None, energy_tol=0.999, detector_pxscale=0.1109):\n",
    "    with fits.open(file_kernel) as hdu:\n",
    "        kernel = hdu[0].data\n",
    "        kernel_header = hdu[0].header\n",
    "    kernel_radius = int((kernel.shape[0] - 1) / 2)\n",
    "\n",
    "    xc = (kernel.shape[1] - 1) / 2\n",
    "    yc = (kernel.shape[0] - 1) / 2\n",
    "    xx, yy = np.meshgrid(np.arange(kernel.shape[1]), np.arange(kernel.shape[0]), indexing='xy')\n",
    "\n",
    "    rr = np.sqrt((xx - xc) ** 2 + (yy - yc) ** 2)\n",
    "\n",
    "    # Calculate radius where enclosed energy starts to exceed 99.9% (or other specified value) of total energy. Trim everything beyond this radius\n",
    "    total_kernel_energy = np.nansum(np.abs(kernel[rr <= kernel_radius]))\n",
    "    for radius in range(kernel_radius):\n",
    "        enclosed_energy = np.nansum(np.abs(kernel[rr <= radius]))\n",
    "        frac_kernel_energy = enclosed_energy / total_kernel_energy\n",
    "        if frac_kernel_energy >= energy_tol:\n",
    "            break\n",
    "    kernel[rr > radius] = 0\n",
    "    trim_offset = kernel_radius - radius\n",
    "    kernel = kernel[trim_offset: -trim_offset, trim_offset: -trim_offset]\n",
    "    print(f\"Trim kernel: old size {kernel_radius*2+1}, new size {kernel_radius*2+1 - 2*trim_offset}\")\n",
    "\n",
    "    # Resample kernel onto the detector pixel scale\n",
    "    pxscale_kernel = WCS(kernel_header).proj_plane_pixel_scales()[0].value*3600\n",
    "    kernel_resample = zoom(kernel, pxscale_kernel/detector_pxscale, order=3)\n",
    "    \n",
    "    # force odd-sized array - the kernel needs to be odd\n",
    "    if kernel_resample.shape[0] % 2 == 0:\n",
    "        kernel_resample = kernel_resample[:-1, :]\n",
    "    if kernel_resample.shape[1] % 2 == 0:\n",
    "        kernel_resample = kernel_resample[:, :-1]\n",
    "    \n",
    "    # repeat circularization to make sure the numerical noise or other artefacts introduced by resampling etc. are minimized (following Aniano+2011 recipe)\n",
    "    kernel_resample = circularize(kernel_resample)\n",
    "    # normalize resampled kernel so the integral == 1\n",
    "    kernel_resample = kernel_resample/np.nansum(kernel_resample)\n",
    "\n",
    "    # save resulting final kernel\n",
    "    fits.writeto(fileout, kernel_resample, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalize kernel\n",
    "pxscale_im = 0.1109  # pxscale of JWST MIRI\n",
    "postprocess_kernel('kernels/F770W_to_F2100W_circ.fits', fileout='kernels/F770W_to_F2100W_fin.fits', detector_pxscale=pxscale_im)\n",
    "\n",
    "# Show the resulting kernel \n",
    "kernel = fits.getdata('kernels/F770W_to_F2100W_fin.fits')\n",
    "norm = ImageNormalize(kernel, interval=PercentileInterval(97))\n",
    "plt.imshow(kernel, origin='lower', interpolation='nearest', norm=norm)\n",
    "plt.title('Kernel F770W -> F2100W')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolve F770W to match F2100W resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final steps are:\n",
    "- Convolve original F770W image using just created F770W->F2100W kernel\n",
    "- Reproject new image to the same wcs grid as for F2100W. \n",
    "- Compare the images. Have the thin filaments gone from $R_{PAH}$ maps? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "from astropy.convolution import convolve_fft\n",
    "galaxy = 'ngc4254'\n",
    "filter = 'f770w'\n",
    "# Read kernel\n",
    "kernel = fits.getdata(os.path.join(kernels_dir, f'{filter.upper()}_to_F2100W_fin.fits'))\n",
    "\n",
    "# Get wcs and shape of target file\n",
    "with fits.open(os.path.join(jwst_data_dir, f'{galaxy}_f2100w.fits')) as hdu:\n",
    "    target_wcs = WCS(hdu['SCI'].header)\n",
    "    target_shape = hdu['SCI'].data.shape\n",
    "        \n",
    "with fits.open(os.path.join(jwst_data_dir, f'{galaxy}_{filter}.fits')) as hdu:\n",
    "    # Do convolution; make sure that all zero values were set to NaNs to not affect the convolution results\n",
    "    hdu['ERR'].data[hdu['SCI'].data == 0] = np.nan\n",
    "    hdu['SCI'].data[hdu['SCI'].data == 0] = np.nan\n",
    "    conv_im = convolve_fft(hdu['SCI'].data, kernel=kernel, preserve_nan=True, fill_value=np.nan)\n",
    "    # Convolve errors (with kernel**2, do not normalize it). This, however, doesn't accound for covariance between pixels\n",
    "    conv_err = np.sqrt(convolve_fft(hdu['ERR'].data**2, kernel**2, \n",
    "                                    preserve_nan=True, normalize_kernel=False))\n",
    "\n",
    "    \n",
    "    hdulist_out = fits.HDUList([fits.PrimaryHDU(header=hdu[0].header)])\n",
    "    # Reprojection to F2100W wcs grid\n",
    "    repr_data, fp = reproject_interp((conv_im, hdu['SCI'].header), output_projection=target_wcs, shape_out=target_shape) \n",
    "    fp = fp.astype(bool)\n",
    "    repr_data[~fp] = np.nan\n",
    "    header = hdu['SCI'].header\n",
    "    header.update(target_wcs.to_header())\n",
    "    hdulist_out.append(fits.ImageHDU(data=repr_data, header=header, name='SCI'))\n",
    "\n",
    "    repr_err = reproject_interp((conv_err, hdu['SCI'].header), output_projection=target_wcs, shape_out=target_shape, return_footprint=False)\n",
    "    repr_err[~fp] = np.nan\n",
    "    header = hdu['ERR'].header\n",
    "    hdulist_out.append(fits.ImageHDU(data=repr_err, header=header, name='ERR'))\n",
    "\n",
    "    hdulist_out.writeto(os.path.join(convolved_data_dir, f'{galaxy}_{filter}_to_f2100w.fits'), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f770w = fits.getdata(os.path.join(convolved_data_dir, f'{galaxy}_f770w_to_f2100w.fits'), 1)\n",
    "f2100w = fits.getdata(os.path.join(jwst_data_dir, f\"{galaxy}_f2100w.fits\"), 1)\n",
    "\n",
    "rpah = f770w/f2100w\n",
    "\n",
    "# Select only part of the images to show (in pixels)\n",
    "xslice = slice(800,1540)\n",
    "yslice = slice(660,1400)\n",
    "\n",
    "\n",
    "# Show images\n",
    "fig = plt.figure(figsize=(21,6))\n",
    "fig.add_subplot(131)\n",
    "norm = ImageNormalize(f770w, stretch=LinearStretch(), interval=PercentileInterval(98.)) \n",
    "plt.imshow(f770w[yslice, xslice], origin='lower', interpolation='nearest', norm=norm, cmap=plt.cm.cubehelix)\n",
    "plt.title(f\"{galaxy}, MIRI F770W (conv. to F2100W)\")\n",
    "plt.colorbar(label='MJy/sr')\n",
    "\n",
    "fig.add_subplot(132)\n",
    "norm = ImageNormalize(f2100w, stretch=LinearStretch(), interval=PercentileInterval(98.))\n",
    "plt.imshow(f2100w[yslice, xslice], origin='lower', interpolation='nearest', norm=norm, cmap=plt.cm.cubehelix)\n",
    "plt.title(f\"{galaxy}, MIRI F2100W\")\n",
    "plt.colorbar(label='MJy/sr')\n",
    "\n",
    "fig.add_subplot(133)\n",
    "plt.imshow(rpah[yslice, xslice], origin='lower', interpolation='nearest', vmin=0.5, vmax=3, cmap=plt.cm.cubehelix)\n",
    "plt.title(f\"{galaxy.upper()}, $R_{{PAH}}$ = F770W/F2100W (matched)\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't consider here F1130W, another PAH-sensitive band, but it is on the disk as well - you are welcome to use it in $R_{PAH}$ calculations. \n",
    "Let's look now at its correlation with the F770W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "f770w = fits.getdata(os.path.join(convolved_data_dir, f'{galaxy}_f770w_to_f2100w.fits'), 'SCI').astype(float)\n",
    "f1130w = fits.getdata(os.path.join(convolved_data_dir, f'{galaxy}_f1130w_to_f2100w.fits'), 'SCI').astype(float)\n",
    "    \n",
    "\n",
    "rec = np.isfinite(f770w) & np.isfinite(f1130w) & (f1130w !=0) & (f770w !=0) & (f770w > -1) & (f1130w > -1) &  (f770w < 10) & (f1130w < 10)\n",
    "sns.jointplot(x=f1130w[rec], y=f770w[rec], kind='hex', vmax=3000)\n",
    "plt.plot([-1,10],[-1,10],'--', color='gray', label='1-to-1 line')\n",
    "cf = np.polyfit(f1130w[rec], f770w[rec], 1)\n",
    "p = np.poly1d(cf)\n",
    "plt.plot([-1,10],p([-1,10]), 'r-', label=f'Fit: slope = {np.round(cf[0],2)}, offset={np.round(cf[1],2)}')\n",
    "plt.xlabel('F1130W, MJy/sr')\n",
    "plt.ylabel('F770W, MJy/sr')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: how to estimate noise level in your images\n",
    "\n",
    "If the noise is uniform across your images (and not correlated between different pixels), it is common way to use standard deviation of the brightness of any empty area in your data.\n",
    "However, we often don't have such empty areas in MIRI images of nearby galaxies.\n",
    "\n",
    "However, because of the tight relation between the mid-IR bands, we can estimate the noise level from the scatter of difference between two rescalled bands (see <a href=https://ui.adsabs.harvard.edu/abs/2023ApJ...944L...9L>Leroy et al. 2023</a> for details):\n",
    "\n",
    "$I_{diff} = I_2 - a*I_1 - b$ => $\\sigma(I_{diff}) = \\sqrt{\\sigma(I_2)^2 + a^2\\sigma(I_1)^2}$ => $\\sigma(I_{obs}) \\sim  \\sigma(I_{diff})/\\sqrt{1+a^2}$\n",
    "(assuming that noise level in $I_1$ and $I_2$ approximately the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = np.isfinite(f770w) & np.isfinite(f1130w) & (f770w > 0) & (f1130w > 0) &  (f770w < 1) & (f1130w < 1)\n",
    "sns.histplot(f770w[rec]-f1130w[rec]*cf[0]-cf[1], binrange=(-0.6,0.6),bins=20)\n",
    "plt.xlabel(f\"F770W - {np.round(cf[0],2)} x F1130W - {np.round(cf[1],2)}, MJy/sr\")\n",
    "print(f\"Estimated noise level: {np.round(np.nanstd((f770w[rec]-f1130w[rec]*cf[0])/np.sqrt(1+cf[0]**2)),2)}\")\n",
    "\n",
    "f770w_err = fits.getdata(os.path.join(jwst_data_dir, f'{galaxy}_f770w.fits'),'ERR')\n",
    "\n",
    "print(f\"Median error propagated through pipeline: {np.round(np.nanmedian(f770w_err[rec]),2):.2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Background matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create kernel for JWST -> Spitzer PSF matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(psf_dir)\n",
    "\n",
    "filters_match = {\"F770W\": \"IRAC4\", \"F2100W\": \"MIPS24\"}\n",
    "# Loop over all bands \n",
    "for filter in filters_match.keys():\n",
    "    # Circularize Spitzer PSF kernels, if this is not done yet\n",
    "    if not os.path.isfile(f'spitzer/PSF_Original_{filters_match[filter]}_circ.fits'):\n",
    "        with fits.open(f'spitzer/PSF_Original_{filters_match[filter]}.fits') as hdu:\n",
    "            psf_circularized = circularize(hdu[0].data[:-1,:-1])\n",
    "            hdu[0].data = psf_circularized\n",
    "            hdu.writeto(f'spitzer/PSF_Original_{filters_match[filter]}_circ.fits', overwrite=True)\n",
    "    file_kernel = os.path.join(kernels_dir, f'{filter}_to_{filters_match[filter]}_circ.fits')\n",
    "    if os.path.isfile(file_kernel):\n",
    "        os.remove(file_kernel)\n",
    "\n",
    "    with fits.open(f'spitzer/PSF_Original_{filters_match[filter]}_circ.fits') as hdu:\n",
    "        pxscale = np.round(hdu[0].header['CD1_1']*3600,4)\n",
    "\n",
    "    os.system(f\"{pypher_cmd} jwst/psf_miri_{filter.lower()}_circ.fits spitzer/PSF_Original_{filters_match[filter]}_circ.fits \"\n",
    "              f\"kernels/{filter}_to_{filters_match[filter]}_circ.fits -r 1e-2\")\n",
    "    \n",
    "    # In this tutorial, we trim kernels at 99% level to allow faster and less resourse demanding convolution. This precision is OK for our purpose\n",
    "    postprocess_kernel(f\"kernels/{filter}_to_{filters_match[filter]}_circ.fits\", fileout=f\"kernels/{filter}_to_{filters_match[filter]}_fin.fits\", energy_tol=0.99, detector_pxscale=0.1109)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolve JWST images: F770W -> IRAC4; F2100W -> MIPS24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.convolution import convolve_fft\n",
    "from astropy.wcs import WCS\n",
    "from scipy.ndimage import zoom\n",
    "from reproject import reproject_interp\n",
    "\n",
    "galaxy = 'ngc4254'\n",
    "filters_match = {\"F770W\": \"IRAC4\", \"F2100W\": \"MIPS24\"}\n",
    "for filter in filters_match:\n",
    "    file_kernel = os.path.join(kernels_dir, f\"{filter}_to_{filters_match[filter]}_fin.fits\")\n",
    "    kernel = fits.getdata(file_kernel)\n",
    "\n",
    "    with fits.open(os.path.join(spitzer_data_dir, f'{galaxy}_{filters_match[filter].lower()}.fits')) as hdu:\n",
    "        target_shape = hdu[0].data.shape\n",
    "        target_wcs = WCS(hdu[0].header)\n",
    "\n",
    "    with fits.open(os.path.join(jwst_data_dir, f'{galaxy}_{filter.lower()}.fits')) as hdu:\n",
    "        print(f'Start with {filter}')\n",
    "        data = hdu['SCI'].data\n",
    "        # mask all zero values before the convolution\n",
    "        hdu['ERR'].data[data==0] = np.nan\n",
    "        data[data == 0] = np.nan\n",
    "        conv_im = convolve_fft(data, kernel=kernel, preserve_nan=True, fill_value=np.nan)\n",
    "        # Convolve errors:\n",
    "        conv_err = np.sqrt(convolve_fft(hdu['ERR'].data**2, kernel**2, preserve_nan=True, normalize_kernel=False))\n",
    "        \n",
    "        hdulist_out = fits.HDUList([fits.PrimaryHDU(header=hdu[0].header)])\n",
    "        # Reprojection to Spitzer wcs grid\n",
    "        repr_data, fp = reproject_interp((conv_im, hdu['SCI'].header), output_projection=target_wcs, shape_out=target_shape) \n",
    "        fp = fp.astype(bool)\n",
    "        repr_data[~fp] = np.nan\n",
    "        header = hdu['SCI'].header\n",
    "        header.update(target_wcs.to_header())\n",
    "        hdulist_out.append(fits.ImageHDU(data=repr_data, header=header, name='SCI'))\n",
    "\n",
    "        repr_err = reproject_interp((conv_err, hdu['SCI'].header), output_projection=target_wcs, shape_out=target_shape, return_footprint=False)\n",
    "        repr_err[~fp] = np.nan\n",
    "        header = hdu['ERR'].header\n",
    "        hdulist_out.append(fits.ImageHDU(data=repr_err, header=header, name='ERR'))\n",
    "\n",
    "        hdulist_out.writeto(os.path.join(convolved_data_dir, f'{galaxy}_{filter}_to_{filters_match[filter]}.fits'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare convolved JWST and Spitzer images and derive background offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy = 'ngc4254'\n",
    "\n",
    "# Read convolved JWST images\n",
    "f770w_conv = fits.getdata(os.path.join(convolved_data_dir, f'{galaxy}_F770W_to_IRAC4.fits'), 1)\n",
    "f2100w_conv = fits.getdata(os.path.join(convolved_data_dir, f'{galaxy}_F2100W_to_MIPS24.fits'), 1)\n",
    "\n",
    "# Read errors\n",
    "f770w_conv_err = fits.getdata(os.path.join(convolved_data_dir, f'{galaxy}_F770W_to_IRAC4.fits'), 2)\n",
    "f2100w_conv_err = fits.getdata(os.path.join(convolved_data_dir, f'{galaxy}_F2100W_to_MIPS24.fits'), 2)\n",
    "\n",
    "# Read Spitzer images\n",
    "irac4 = fits.getdata(os.path.join(spitzer_data_dir, f'{galaxy}_irac4.fits'))\n",
    "mips24 = fits.getdata(os.path.join(spitzer_data_dir, f'{galaxy}_mips24.fits'))\n",
    "\n",
    "# Select only part of the images to show (in pixels)\n",
    "xslice_f770w = slice(720,1020)\n",
    "yslice_f770w = slice(450,750)\n",
    "xslice_f2100w = slice(100,250)\n",
    "yslice_f2100w = slice(100,250)\n",
    "\n",
    "\n",
    "# Show images\n",
    "fig = plt.figure(figsize=(15,12))\n",
    "fig.add_subplot(221)\n",
    "norm = ImageNormalize(f770w_conv, stretch=LinearStretch(), interval=PercentileInterval(98.)) \n",
    "plt.imshow(f770w_conv[yslice_f770w, xslice_f770w], origin='lower', interpolation='nearest', norm=norm, cmap=plt.cm.cubehelix)\n",
    "plt.title(f\"{galaxy}, MIRI F770W (conv. to IRAC 8.0$\\mu$)\")\n",
    "plt.colorbar(label='MJy/sr')\n",
    "\n",
    "fig.add_subplot(222)\n",
    "plt.imshow(irac4[yslice_f770w, xslice_f770w], origin='lower', interpolation='nearest', norm=norm, cmap=plt.cm.cubehelix)\n",
    "plt.title(f\"{galaxy},IRAC 8.0$\\mu$\")\n",
    "plt.colorbar(label='MJy/sr')\n",
    "\n",
    "fig.add_subplot(223)\n",
    "norm = ImageNormalize(f2100w_conv, stretch=LinearStretch(), interval=PercentileInterval(98.))\n",
    "plt.imshow(f2100w_conv[yslice_f2100w, xslice_f2100w], origin='lower', interpolation='nearest', norm=norm, cmap=plt.cm.cubehelix)\n",
    "plt.title(f\"{galaxy}, MIRI F2100W (conv. to MIPS 24$\\mu$)\")\n",
    "plt.colorbar(label='MJy/sr')\n",
    "\n",
    "fig.add_subplot(224)\n",
    "plt.imshow(mips24[yslice_f2100w, xslice_f2100w], origin='lower', interpolation='nearest', norm=norm, cmap=plt.cm.cubehelix)\n",
    "plt.title(f\"{galaxy},MIPS 24$\\mu$\")\n",
    "plt.colorbar(label='MJy/sr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ measure background offset in MIRI images. Working solution is given below. \n",
    "Steps:\n",
    "1. Look at correlations of F770W vs IRAC4 and F2100W vs MIPS24. They must follow each other very well \n",
    "2. Fit linear regression to the data. Derived offset will be your desired value, because:\n",
    "    - if background offset is 0, then $JWST_{obs} = JWST_{true} \\propto Spitzer_{true}$ because of the similarity of the considered bands\n",
    "    - With background offset, $JWST_{obs} = JWST_{true} + bgr \\simeq A * Spitzer_{true} + bgr$\n",
    "\n",
    "Hints:\n",
    "- Pre-filter your data by removing NaNs, zeros and values with low S/N (e.g., less than 10)\n",
    "- For linear regression, better exclude the brightest pixels\n",
    "- Here you can use numpy.polyfit to fit regression. However, in general, algoritms that fit linear regression with outlier rejections produce more robust results. As example, <a href=\"https://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html#sphx-glr-auto-examples-linear-model-plot-ransac-py\">RANSAC</a> algorithm from _sklearn_ works very well\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import linear_model to do robust linear regression with outlier rejection\n",
    "from sklearn import linear_model \n",
    "\n",
    "# Do analysis first for F770W and IRAC 8.0 mkm, and then for F2100W and MIPS 24 mkm pairs. \n",
    "# Put data into lists to loop over them\n",
    "all_x = [irac4, mips24]\n",
    "all_y = [f770w_conv, f2100w_conv]\n",
    "all_yerr = [f770w_conv_err, f2100w_conv_err]\n",
    "\n",
    "filter_x = [r'IRAC 8.0$\\mu$', r'MIPS 24$\\mu$']\n",
    "filter_y = ['MIRI F770W', 'MIRI F2100W']\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "\n",
    "# loop over these lists\n",
    "for ind in range(len(all_x)):\n",
    "    x = all_x[ind]\n",
    "    y = all_y[ind]\n",
    "    yerr = all_yerr[ind]\n",
    "    # mask data by removing NaNs, zeros, and values with low S/N < 10 in JWST bands\n",
    "    # note that S/N < 5 will remove all negative values. At the same time, some of these points could be real (if background offset is negative). \n",
    "    # However, this doesn't affect the results of regression\n",
    "    mask = np.isfinite(x) & np.isfinite(y) & (x != 0) & (y !=0) #& np.isfinite(yerr) & (y/yerr >= 10)\n",
    "\n",
    "    # plot selected data. Limit to the moderate brightness levels (e.g. below 15 MJy/st)\n",
    "    limits = np.array([-0.5, 15])\n",
    "    plt.sca(ax[ind])\n",
    "    plt.scatter(x[mask], y[mask], alpha=0.1)\n",
    "    plt.xlabel(filter_x[ind])\n",
    "    plt.ylabel(filter_y[ind])\n",
    "    plt.xlim(limits)\n",
    "    plt.ylim(limits)\n",
    "    \n",
    "    # remove bright pixels from the fitting\n",
    "    mask_for_fit = mask & (x < 15) & (y < 15)\n",
    "    \n",
    "    # fit linear regression using RANSAC algorithm with outlier rejection\n",
    "    ransac = linear_model.RANSACRegressor()\n",
    "    ransac.fit(x[mask_for_fit].reshape(-1,1), y[mask_for_fit].reshape(-1,1))\n",
    "\n",
    "    cf = ransac.estimator_.coef_[0][0]     # normalization coefficient between Spitzer and JWST bands\n",
    "    bgr = ransac.estimator_.intercept_[0]  # background offset level\n",
    "    \n",
    "    # overplot regression\n",
    "    plt.plot(limits, cf*limits + bgr,'r-')\n",
    "    \n",
    "    print(f\"Derived background offset for {galaxy} {filter_y[ind]}: {np.round(bgr,2)} MJy/sr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our JWST images can have measurable background offsets. While it is not critical for analysis of the bright (in mid-IR) regions, accounting for such offset is important for evaluating the mid-IR brightnesses, and especially their ratios (e.g. $R_{PAH}$) in diffuse ISM.\n",
    "\n",
    "In order to correct for the offest, we can subtract the derived values from the original (and convolved) images. \n",
    "From that point, we will use in the further analysis background-corrected images in directories defined in _jwst\\_data\\_bgrcorr\\_dir_ and _convolved\\_bgrcorr\\_data\\_dir_ variables (_bgrcorr_ and _convolved\\_bgrcorr_ in _PHANGS-JWST_ folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Mid-IR bands vs $\\rm H\\alpha$ correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to directly compare images in different wavelenght and from different instuments, we need first to math their PSF. \n",
    "This time, we compare the JWST/MIRI and MUSE Halpha images (from PHANGS-JWST and PHANGS-MUSE). \n",
    "\n",
    "The MUSE data were pre-processed in a way that resulting PSF is homogeneous Gaussian accross the field of view and wavelengths (see details in <a href='https://ui.adsabs.harvard.edu/abs/2022A%26A...659A.191E'>Emsellem et al. 2022</a>). Therefore, in order to convolve JWST images to the MUSE resolution, we can follow these steps:\n",
    "\n",
    "1. Create Gaussian PSF (_already available in PSF/gaus directory_)\n",
    "    - FWHM (in arcsec) of this PSF must be equal to that in the data (it is specified in the filenames)\n",
    "    - For simplicity, make Gaussian PSF array of the same shape and pixel size as the source (JWST) PSF\n",
    "2. Use pypher (or other soft) to create kernels from circularized JWST PSFs to Gaussian PSF. \n",
    "3. Postprocess kernel (trim, circularize, resample to the original pixelscale)\n",
    "4. Convolve images (F770W and F2100W) to the MUSE $H\\alpha$ resolution\n",
    "5. Reproject the convolved images to the MUSE wcs grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ try to implement these steps. Working solution is given bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy = 'ngc4254' # choose any galaxy among available\n",
    "\n",
    "# Use this directories to not break file structure for following excersises \n",
    "convolved_data_dir = convolved_bgrcorr_data_dir # redefine to use the background corrected data\n",
    "jwst_data_dir = jwst_bgrcorr_data_dir # redefine to use the background corrected data\n",
    "jwst_psf_dir = os.path.join(psf_dir, 'jwst') # directory with jwst PSFs\n",
    "muse_psf_dir = os.path.join(psf_dir, 'gauss') # directory with jwst PSFs\n",
    "# kernels_dir =  os.path.join(psf_dir,'kernels') # directory with kernels; already definded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create gaussian PSF based on the shape and pixscale of the JWST PSFs ===\n",
    "#  Note: all JWST PSFs considered here have the same shape and pixel scale\n",
    "all_jwst_psfs = glob.glob(os.path.join(jwst_psf_dir,\"*_circ.fits\"))\n",
    "\n",
    "# Halpha file for current galaxy\n",
    "file_ha = glob.glob(os.path.join(muse_data_dir, f'{galaxy}*obs.fits'))[0]\n",
    "\n",
    "# get PSF FWHM from the filename and convert it to sigma\n",
    "g_fwhm = float(re.search(\"(\\d\\.\\d\\d)\",file_ha).group()) \n",
    "g_sigma = g_fwhm / 2 / np.sqrt(2 * np.log(2))\n",
    "\n",
    "# === Create gaussian PSF with the same shape and pixscale as the JWST PSFs ===\n",
    "with fits.open(all_jwst_psfs[0]) as hdu:\n",
    "    shape = hdu[0].shape\n",
    "    pxscale = hdu[0].header.get('pixelscl')\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(shape[1]) * pxscale, np.arange(shape[0]) * pxscale, indexing='xy')\n",
    "yc, xc = shape[1]*pxscale/2, shape[0]*pxscale/2\n",
    "rad = np.min([xc, yc])\n",
    "\n",
    "g_psf = np.exp( - ((xx - xc) ** 2 + (yy - yc) ** 2) /  2 / g_sigma ** 2)\n",
    "g_psf[((xx - xc) ** 2 + (yy - yc) ** 2) > (rad ** 2)] = 0\n",
    "g_psf = g_psf / np.nansum(g_psf) # normalize PSF\n",
    "\n",
    "f_psf_ha = os.path.join(muse_psf_dir, f'gaus_{g_fwhm}.fits') # where to save this just created PSF\n",
    "fits.writeto(f_psf_ha, g_psf, overwrite=True)\n",
    "plt.imshow(g_psf, origin='lower', interpolation='nearest')\n",
    "plt.title(f\"PSF: 2D Gaussian, FWHM={g_fwhm}\")\n",
    "\n",
    "\n",
    "# === Create kernels with pypher for all available circularized and oversampled JWST PSFs === \n",
    "kernels = {}\n",
    "os.system(f\"{addpixscl_cmd} {f_psf_ha} {pxscale}\")\n",
    "for f_psf_jwst in (all_jwst_psfs):\n",
    "    filter = re.search('(f\\d+w)',f_psf_jwst).group() # get name of the current MIRI filter from the filename\n",
    "    file_kernel = os.path.join(kernels_dir, f'{filter}_to_gaus_{g_fwhm}_circ.fits') # name of the kernel for JWST -> MUSE tranformation\n",
    "    print(file_kernel)\n",
    "    # remove old kernel, if already exists\n",
    "    if os.path.isfile(file_kernel):\n",
    "        os.remove(file_kernel)\n",
    "\n",
    "    os.system(f\"{pypher_cmd} {f_psf_jwst} {f_psf_ha} {file_kernel} -r 0.01\") # Run pypher \n",
    "    \n",
    "\n",
    "    # Post-process kernel: trim, circularize, resample, normalize\n",
    "    postprocess_kernel(file_kernel, fileout=file_kernel.split('_circ.fits')[0]+'_fin.fits')\n",
    "    \n",
    "    # Add just created kernel to the dictionary for further usage in a loop\n",
    "    kernels[filter] = fits.getdata(file_kernel)\n",
    "\n",
    "# Loop over all created kernels and visualize them\n",
    "fig = plt.figure(figsize=(21,6))\n",
    "for ind, filter in enumerate(kernels.keys()):\n",
    "    ax = fig.add_subplot(100+len(kernels.keys())*10+ind+1)\n",
    "    norm = ImageNormalize(kernels[filter], interval=PercentileInterval(99.))\n",
    "    plt.imshow(kernels[filter], origin='lower', interpolation='nearest', norm=norm)\n",
    "    plt.title(f'{filter} to Gauss FWHM={g_fwhm}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do convolution with the kernels above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "from astropy.convolution import convolve_fft\n",
    "\n",
    "files_jwst = glob.glob(os.path.join(jwst_bgrcorr_data_dir,f\"{galaxy.lower()}*w.fits\"))\n",
    "\n",
    "# filename, PSF FWHM, wcs and shape of Halpha file for this galaxy\n",
    "file_ha = glob.glob(os.path.join(muse_data_dir, f\"{galaxy}*obs.fits\"))[0] \n",
    "g_fwhm = float(re.search(\"(\\d\\.\\d\\d)\",file_ha).group()) # FWHM of MUSE PSF, from the filename\n",
    "wcs_ha = WCS(fits.getheader(file_ha))\n",
    "shape_ha = fits.getdata(file_ha).shape\n",
    "\n",
    "for f in (files_jwst):\n",
    "    # find name of the filter in the filename\n",
    "    filter = re.search('(f\\d+w)',f).group()\n",
    "\n",
    "    # Read kernel\n",
    "    file_kernel = os.path.join(kernels_dir, f\"{filter}_to_gaus_{g_fwhm}_fin.fits\")\n",
    "    kernel = fits.getdata(file_kernel)\n",
    "\n",
    "    # Read JWST image and process it\n",
    "    with fits.open(f) as hdu:\n",
    "        # make sure that all zero values are masked\n",
    "        data = hdu['SCI'].data\n",
    "        data[data == 0] = np.nan\n",
    "        conv_im = convolve_fft(data, kernel, preserve_nan=True, fill_value=np.nan)\n",
    "\n",
    "        # Convolve errors\n",
    "        conv_err = np.sqrt(convolve_fft(hdu['ERR'].data**2, kernel**2, preserve_nan=True, fill_value=np.nan, normalize_kernel=False))\n",
    "        \n",
    "        # Reprojection to MUSE wcs grid\n",
    "        repr_im, fp = reproject_interp((conv_im, hdu['SCI'].header), wcs_ha, shape_ha)\n",
    "        fp = fp.astype(bool)\n",
    "        repr_im[~fp] = np.nan\n",
    "        repr_err = reproject_interp((conv_err, hdu['SCI'].header), wcs_ha, shape_ha, return_footprint=False)\n",
    "        repr_err[~fp] = np.nan\n",
    "        \n",
    "        # Save to new fits\n",
    "        header_sci = hdu['SCI'].header\n",
    "        header_sci.update(wcs_ha.to_header())\n",
    "        header_out = fits.HDUList([fits.PrimaryHDU(header=hdu[0].header), \n",
    "                                   fits.ImageHDU(data=repr_im, header=header_sci, name=\"SCI\"),\n",
    "                                   fits.ImageHDU(data=repr_err, header=hdu['ERR'].header, name='ERR')])\n",
    "\n",
    "        header_out.writeto(os.path.join(convolved_bgrcorr_data_dir, f'{galaxy}_{filter}_to_MUSE.fits'), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ How Mid-IR bands depend on $H\\alpha$? Do they trace SFR or not? What about $R_{PAH}$? \n",
    "(Working solution are below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import seaborn as sns\n",
    "galaxy = 'ngc4254'\n",
    "file_ha = glob.glob(os.path.join(muse_data_dir, f\"{galaxy}*corr.fits\"))[0]\n",
    "img_muse = fits.getdata(file_ha)\n",
    "img_muse_err = fits.getdata(file_ha, 1)\n",
    "img_jwst_770 = fits.getdata(os.path.join(convolved_bgrcorr_data_dir, f'{galaxy}_f770w_to_MUSE.fits'),1)\n",
    "img_jwst_2100 = fits.getdata(os.path.join(convolved_bgrcorr_data_dir, f'{galaxy}_f2100w_to_MUSE.fits'),1)\n",
    "img_jwst_770_err = fits.getdata(os.path.join(convolved_bgrcorr_data_dir, f'{galaxy}_f770w_to_MUSE.fits'),2)\n",
    "img_jwst_2100_err = fits.getdata(os.path.join(convolved_bgrcorr_data_dir, f'{galaxy}_f2100w_to_MUSE.fits'),2)\n",
    "\n",
    "rpah = img_jwst_770/img_jwst_2100\n",
    "\n",
    "\n",
    "rec = np.isfinite(img_jwst_2100) & np.isfinite(img_jwst_770) & np.isfinite(img_muse) & (img_muse/img_muse_err > 10) & (img_jwst_770 > 0.1) & (img_jwst_2100 > 0.1) & (img_muse>1e-18)\n",
    "\n",
    "x = np.log10(img_muse[rec])\n",
    "y = np.log10(img_jwst_770[rec])\n",
    "sns.jointplot(x=x,y=y, kind='hex',vmax=1500)\n",
    "plt.ylabel(r'$\\log$ I(F770W), MJy/sr')\n",
    "plt.xlabel(r'$\\log$ I(H$\\alpha$), erg/s/cm$^2$/arcsec$^2$')\n",
    "\n",
    "y = np.log10(img_jwst_2100[rec])\n",
    "sns.jointplot(x=x,y=y, kind='hex',vmax=1500)\n",
    "plt.ylabel(r'$\\log$ I(F2100W), MJy/sr')\n",
    "plt.xlabel(r'$\\log$I(H$\\alpha$), erg/s/cm$^2$/arcsec$^2$')\n",
    "\n",
    "rec = rec & (rpah < 6)\n",
    "x = np.log10(img_muse[rec])\n",
    "y = (rpah[rec])\n",
    "sns.jointplot(x=x,y=y, kind='hex',vmax=1500)\n",
    "plt.ylabel(r'$R_{PAH}$')\n",
    "plt.xlabel(r'$\\log$I(H$\\alpha$), erg/s/cm$^2$/arcsec$^2$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mid-IR bands (esp. F2100W) correlate very well with the extinction-corrected $H\\alpha$ and thus can be served as perfect indicators of embedded star-formation (e.g. <a href=https://ui.adsabs.harvard.edu/abs/2023arXiv230611811B>Belfiore et al. (2023)</a>)\n",
    "- Meanwhile, $R_{PAH}$ decrease with $H\\alpha$ brightness. We will explore this below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. $\\rm R_{PAH}$ in different galaxies\n",
    "\n",
    "__Excersise:__ Are the any differences in $R_{PAH}$ in different galaxies? \n",
    "- Compare $R_{PAH}$ distribution (at MUSE resolution) for all 4 galaxies. How would you interpret the results?\n",
    "- Plot $R_{PAH}$ vs F($H\\alpha$) for these 4 galaxies. Are there any differences in comparison to the plot obtained above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average value in bins for scatter plots\n",
    "def calc_mean(x, y, nbins=30, bins = None):\n",
    "    \"\"\"\n",
    "    Divide the x axis into sections and return groups of y based on its x value\n",
    "    \"\"\"\n",
    "    if bins is None:\n",
    "        bins = np.linspace(np.nanmin(x), np.nanmax(x), nbins)\n",
    "\n",
    "    bin_space = (bins[-1] - bins[0])/(len(bins)-1)/2\n",
    "\n",
    "    indicies = np.digitize(x, bins + bin_space)\n",
    "    mean = []\n",
    "    for i in range(0, len(bins)):\n",
    "        if np.sum(indicies==i) < 10:\n",
    "            mean.append(np.nan)\n",
    "        else:\n",
    "            mean.append(np.nanmean(y[indicies==i]))\n",
    "    return bins, mean\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all galaxies available in jwst directory\n",
    "all_jwst_files = glob.glob(os.path.join(jwst_bgrcorr_data_dir, '*_f2100w.fits'))\n",
    "gal_names = np.unique([f.split('/')[-1].split('_')[0] for f in all_jwst_files])\n",
    "\n",
    "SN_lim = 5\n",
    "SN_lim_muse=15\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,8))\n",
    "for ind, gal in enumerate(gal_names):\n",
    "    \n",
    "    img_770 = fits.getdata(os.path.join(convolved_bgrcorr_data_dir, f'{gal}_f770w_to_MUSE.fits'),1)\n",
    "    # Read Halpha image corrected for extinction\n",
    "    file_ha = glob.glob(os.path.join(muse_data_dir, f'{gal}*corr.fits'))[0] \n",
    "    img_ha = fits.getdata(file_ha)\n",
    "    img_ha_err = fits.getdata(file_ha, 1)\n",
    "    img_2100 = fits.getdata(os.path.join(convolved_bgrcorr_data_dir, f'{gal}_f2100w_to_MUSE.fits'),1)\n",
    "    img_770_err = fits.getdata(os.path.join(convolved_bgrcorr_data_dir, f'{gal}_f770w_to_MUSE.fits'),2)\n",
    "    img_2100_err = fits.getdata(os.path.join(convolved_bgrcorr_data_dir, f'{gal}_f2100w_to_MUSE.fits'),2)\n",
    "\n",
    "    # select only the pixels with S/N above low limits\n",
    "    mask = np.isfinite(img_770) & np.isfinite(img_2100) & (img_770/img_770_err > SN_lim) & (img_2100/img_2100_err > SN_lim) \n",
    "\n",
    "    rpah = img_770/img_2100\n",
    "    plt.sca(ax[0])\n",
    "    sns.histplot(rpah[mask], label=gal,binrange=[0,3], ax=ax[0])\n",
    "    cm = plt.get_cmap()\n",
    "\n",
    "    if ind == (len(gal_names) - 1):\n",
    "        plt.xlabel(r\"$R_{PAH}$\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.sca(ax[1])\n",
    "    mask = mask & np.isfinite(img_ha) & (img_ha/img_ha_err > SN_lim_muse)\n",
    "    \n",
    "    # calculate average values in bins\n",
    "    mx, my = calc_mean(np.log10(img_ha[mask]), rpah[mask], nbins=30, bins = None)\n",
    "    plt.plot(mx,my, '-', linewidth=3, label=gal)\n",
    "    plt.scatter(np.log10(img_ha[mask]), rpah[mask], facecolors='white', edgecolors='gray', s = 5, lw = 0.7, alpha=0.5, rasterized=True)\n",
    "\n",
    "    if ind == (len(gal_names) - 1):\n",
    "        plt.xlabel(r\"$\\log$I(H$\\alpha$), erg/s/cm$^2$/arcsec$^2$\")\n",
    "        plt.ylabel(r\"$R_{PAH}$\")\n",
    "        plt.legend()\n",
    "        plt.ylim(0,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots, we can see two things: \n",
    " - PAH fraction (traced by $R_{PAH}$) is typically lower in IC5332 than in other galaxies. This galaxy has lowest metallicity among these four ($Z \\sim 0.4Z_\\odot$ vs $Z \\sim 0.8 Z_\\odot$). Lower metallicity is likely reason for lower PAH fraction in IC5332 (see more details in <a href=\"https://ui.adsabs.harvard.edu/abs/2023ApJ...944L..11C\">Chastenet et al. 2023</a>)\n",
    " - $R_{PAH}$ decreases in the regions bright in $H\\alpha$. This is likely due to their destruction by extreme UV radiation (see <a href=\"https://ui.adsabs.harvard.edu/abs/2023ApJ...944L..16E\">Egorov et al. 2023</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. $R_{PAH}$ in HII regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better explore the declining $R_{PAH}$ with $H\\alpha$, lets focus on HII regions only. \n",
    "<a href=\"https://ui.adsabs.harvard.edu/abs/2023MNRAS.520.4902G\">Groves et al. (2023)</a> compiled a catalog of the nebulae in all 19 PHANGS-MUSE galaxies. It contains their measured (e.g. line fluxes etc.) and derived (e.g. metallicity, ionization parameter etc.) properties, together with the masks defining the borders of the regions. \n",
    "The full catalog and masks for 4 galaxies are avaiable in _PHANGS-MUSE_ folder.\n",
    "Masks has values _region\\_ID_ (if the pixel is related to corresponding region from the catalog) or 0 (if the pixel is not in HII region). The catalog is considered below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "hii_catalog = Table.read(os.path.join(muse_data_dir,'Nebulae_catalogue.fits'))\n",
    "hii_catalog.info()\n",
    "\n",
    "# select only nebulae that were classified as HII regions (not SNRs)\n",
    "hii_catalog = hii_catalog[hii_catalog['HII_class_v3'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal = 'ngc628'\n",
    "hii_mask = fits.getdata(os.path.join(muse_data_dir, f'{gal}_HIIregs_mask.fits')).astype(float)\n",
    "hii_mask[hii_mask > 0] = 100\n",
    "\n",
    "f770w = fits.getdata(os.path.join(convolved_bgrcorr_data_dir, f'{gal}_f770w_to_MUSE.fits'), 'SCI')\n",
    "f2100w = fits.getdata(os.path.join(convolved_bgrcorr_data_dir, f'{gal}_f2100w_to_MUSE.fits'), 'SCI')\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(20,12))\n",
    "plt.sca(ax1)\n",
    "norm=ImageNormalize(f770w, interval=PercentileInterval(97.), stretch=AsinhStretch())\n",
    "plt.imshow(f770w, origin='lower', cmap=plt.cm.cubehelix, norm=norm)\n",
    "plt.colorbar(label='F770W')\n",
    "plt.contour(hii_mask, levels=[99])\n",
    "plt.xlim(400,1000)\n",
    "plt.ylim(400,1200)\n",
    "\n",
    "plt.sca(ax2)\n",
    "rpah=f770w/f2100w\n",
    "# norm=ImageNormalize(f2100w, interval=PercentileInterval(97.), stretch=AsinhStretch())\n",
    "plt.imshow(rpah, origin='lower', cmap=plt.cm.cubehelix, vmin=0., vmax=4.)\n",
    "plt.colorbar(label='$R_{PAH}$')\n",
    "plt.contour(hii_mask, levels=[99])\n",
    "plt.xlim(400,1000)\n",
    "plt.ylim(400,1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Excersice__: How PAHs behave in HII regions?\n",
    "1. Select HII regions from this catalog for 4 galaxies presented here. Among them, select only regions that satisfy following criteria:\n",
    "- Large enough (_region_circ_rad_ > 1 arcsec) to be considered as resolved (not much affected by diffuse ISM)\n",
    "- S/N > 15 in $H\\alpha$ line and S/N > 5 in other lines of our interest here ([SII]6716, 6730; [SIII]9068)\n",
    "2. Extract gas-phase metallicity (met_scal) and ionization parameter (logarithm of ratio of [SIII]9068/[SII]6716+6730) for the selected regions. \n",
    "3. Measure $R_{PAH}$ for these regions (from the data convolved to MUSE resolution). Just integrate the flux in the pixels corresponding to the masks for each region\n",
    "4. Plot $R_{PAH}$ vs ionization parameter (and metallicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all galaxies available in jwst directory\n",
    "all_jwst_files = glob.glob(os.path.join(jwst_bgrcorr_data_dir, '*_f2100w.fits'))\n",
    "gal_names = np.unique([f.split('/')[-1].split('_')[0] for f in all_jwst_files])\n",
    "\n",
    "snr_lim = 10\n",
    "\n",
    "rpahs = []\n",
    "mets = []\n",
    "ips = []\n",
    "\n",
    "for gal in gal_names:\n",
    "    if gal == 'ngc628':\n",
    "        gal_in_cat = 'NGC0628'\n",
    "    else:\n",
    "        gal_in_cat = gal.upper()\n",
    "    # select only those rows related to HII regions from current galaxy\n",
    "    cat_selected = hii_catalog[hii_catalog['gal_name'] == gal_in_cat]\n",
    "    # Get list of all HII region IDs in this galaxy\n",
    "    region_ids = np.unique(cat_selected['region_ID'])\n",
    "\n",
    "    # read HII regions mask\n",
    "    hii_mask = fits.getdata(os.path.join(muse_data_dir, f'{gal}_HIIregs_mask.fits'))\n",
    "\n",
    "    # read f770w and f2100w images (convolved and reprojected to MUSE grid)\n",
    "    with fits.open(os.path.join(convolved_bgrcorr_data_dir, f'{gal}_f770w_to_MUSE.fits')) as hdu:\n",
    "        f770w = hdu['SCI'].data\n",
    "        f770w_err = hdu['ERR'].data\n",
    "        \n",
    "    with fits.open(os.path.join(convolved_bgrcorr_data_dir, f'{gal}_f2100w_to_MUSE.fits')) as hdu:\n",
    "        f2100w = hdu['SCI'].data\n",
    "        f2100w_err = hdu['ERR'].data\n",
    "\n",
    "\n",
    "    # iterate over the regions:\n",
    "    for reg_id in region_ids:\n",
    "        cur_mask = hii_mask ==reg_id\n",
    "\n",
    "        # skip the regions if they span less than 10 pixels\n",
    "        if np.nansum(cur_mask) < 10:\n",
    "            continue\n",
    "\n",
    "        cur_cat_selection = cat_selected['region_ID'] == reg_id\n",
    "        # Do not consider regions with diameter less than 2 arcsec\n",
    "        if cat_selected['region_circ_rad'][cur_cat_selection] < 1.:\n",
    "            continue\n",
    "        # skip the regions with low S/N in any involved line or JWST MIRI band\n",
    "        snr_ha = (cat_selected['HA6562_FLUX_CORR']/cat_selected['HA6562_FLUX_CORR_ERR'])[cur_cat_selection]\n",
    "        if snr_ha < 15:\n",
    "            continue\n",
    "        snr_s3 = (cat_selected['SIII9068_FLUX_CORR']/cat_selected['SIII9068_FLUX_CORR_ERR'])[cur_cat_selection]\n",
    "        if snr_s3 < 5:\n",
    "            continue\n",
    "        snr_s2 = (cat_selected['SII6716_FLUX_CORR']/cat_selected['SII6716_FLUX_CORR_ERR'])[cur_cat_selection]\n",
    "        if snr_s2 < 5:\n",
    "            continue\n",
    "        snr_s2 = (cat_selected['SII6730_FLUX_CORR']/cat_selected['SII6730_FLUX_CORR_ERR'])[cur_cat_selection]\n",
    "        if snr_s2 < 5:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        f770w_cur = np.nansum(f770w[cur_mask])\n",
    "        f770w_cur_err = np.sqrt(np.nansum(f770w_err[cur_mask]**2))\n",
    "        if f770w_cur/f770w_cur_err < snr_lim:\n",
    "            continue\n",
    "        \n",
    "        f2100w_cur = np.nansum(f2100w[cur_mask])\n",
    "        f2100w_cur_err = np.sqrt(np.nansum(f2100w_err[cur_mask]**2))\n",
    "        if f2100w_cur/f2100w_cur_err < snr_lim:\n",
    "            continue\n",
    "        \n",
    "        # Calculate R_PAH for the selected regions\n",
    "        rpah_cur = (f770w_cur)/f2100w_cur\n",
    "        \n",
    "        # get metallicity from the catalog\n",
    "        met_cur = cat_selected['met_scal'][cur_cat_selection]\n",
    "        # get empirical tracer of the ionization parameter ([SIII]/[SII])\n",
    "        ip_cur = np.log10((cat_selected['SIII9068_FLUX_CORR']/(cat_selected['SII6716_FLUX_CORR'] + cat_selected['SII6730_FLUX_CORR']))[cur_cat_selection])\n",
    "\n",
    "        rpahs.append(rpah_cur)\n",
    "        mets.append(met_cur)\n",
    "        ips.append(ip_cur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpahs = np.squeeze(np.array(rpahs))\n",
    "mets = np.squeeze(np.array(mets))\n",
    "ips = np.squeeze(np.array(ips))\n",
    "rec = np.isfinite(ips) & np.isfinite(rpahs) & (rpahs < 3.5) & (mets > 8.2) & (ips > -2.) & (ips < 0)\n",
    "\n",
    "# sns.histplot(x=(ips[rec]), y=rpahs[rec])\n",
    "\n",
    "plt.scatter(ips[rec], rpahs[rec], c=mets[rec], s=10,vmin=8.45)\n",
    "plt.colorbar(label=r'12+$\\log$(O/H)')\n",
    "\n",
    "x,y=calc_mean(ips[rec], rpahs[rec], nbins=15)\n",
    "plt.plot(x,y,'r-')\n",
    "plt.ylim(0,3.)\n",
    "plt.xlim(-2.,0)\n",
    "plt.xlabel(r'$\\log$([SIII]9068/[SII]6717+6731)')\n",
    "plt.ylabel(r'$R_{PAH}$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependence on the metallicity is not obvious, but $R_{PAH}$ strongly depends on the [SIII]/[SII] - empirical tracer of ionization parameter. Probably, PAHs been destroyed very efficiently in HII regions, independently on the ISM metallicity. More careful analysis of this in <a href=\"https://ui.adsabs.harvard.edu/abs/2023ApJ...944L..16E\">Egorov et al. 2023</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus. Ionization structure of ISM and just beautiful images => not working on datalabs :( "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short demonstration of how to make nice color images combined multiwavelength datasets. \n",
    "<a href='https://github.com/pjcigan/multicolorfits'>multicolorfits</a> is very useful for this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multicolorfits as mcf\n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you will use __multicolorfits__ in this notebook - make sure to explicitely turn on \"inline\" mode for matplolib after importing it. Otherwise __multicolorfits__ will try to run GUI interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "galaxy = 'ngc628'\n",
    "\n",
    "\n",
    "with fits.open(os.path.join(jwst_bgrcorr_data_dir, f'{galaxy}_f2100w.fits')) as hdu:\n",
    "    f2100w_head = hdu['SCI'].header\n",
    "    wcs_f2100w = WCS(hdu['SCI'].header)\n",
    "    im_f2100w = hdu['SCI'].data\n",
    "\n",
    "with fits.open(os.path.join(jwst_bgrcorr_data_dir, f'{galaxy}_f770w.fits')) as hdu:\n",
    "    wcs_f770w = WCS(hdu['SCI'].header)\n",
    "    im_f770w = hdu['SCI'].data\n",
    "    im_f770w = reproject_interp((im_f770w,wcs_f770w), wcs_f2100w, im_f2100w.shape, return_footprint=False)\n",
    "    \n",
    "f_ha = glob.glob(os.path.join(muse_data_dir, f'{galaxy}*obs.fits'))[0]\n",
    "with fits.open(f_ha) as hdu:\n",
    "    wcs_ha = WCS(hdu[0].header)\n",
    "    im_ha = hdu[0].data\n",
    "    im_ha = reproject_interp((im_ha,wcs_ha), wcs_f2100w, im_f2100w.shape, return_footprint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radec = SkyCoord(\"12h18m52s +14d25m50s\")\n",
    "radec = SkyCoord(\"1h36m42s +15d46m37s\")\n",
    "size = 1.8*u.arcmin\n",
    "im_ha_cout = Cutout2D(im_ha, radec, size=size, wcs=wcs_f2100w)\n",
    "im_f770w_cout = Cutout2D(im_f770w, radec, size=size, wcs=wcs_f2100w)\n",
    "im_f2100w_cout = Cutout2D(im_f2100w, radec, size=size, wcs=wcs_f2100w)\n",
    "\n",
    "\n",
    "ha_greyRGB=mcf.greyRGBize_image(im_ha_cout.data, rescalefn='asinh', scaletype='perc', \n",
    "                                min_max=[2.,97.], gamma=3.2, checkscale=True)\n",
    "\n",
    "f770w_greyRGB=mcf.greyRGBize_image(im_f770w_cout.data, rescalefn='asinh', scaletype='perc', \n",
    "                                    min_max=[5.,99.], gamma=2.2, checkscale=True)\n",
    "f2100w_greyRGB=mcf.greyRGBize_image(im_f2100w_cout.data, rescalefn='asinh', scaletype='perc', \n",
    "                                    min_max=[5,99.5], gamma=2.2, checkscale=True)\n",
    "\n",
    "ha_color=mcf.colorize_image(ha_greyRGB, '#c05210', colorintype='hex', gammacorr_color=2.2)\n",
    "f2100w_color=mcf.colorize_image(f2100w_greyRGB, '#FF2211', colorintype='hex', gammacorr_color=2.2)\n",
    "f770w_color=mcf.colorize_image(f770w_greyRGB, '#0072c6', colorintype='hex', gammacorr_color=2.2)\n",
    "\n",
    "img=mcf.combine_multicolor([ha_color,f770w_color, f2100w_greyRGB],gamma=2.2);\n",
    "\n",
    "mcf.plotsinglemulticolorRGB(img, im_f2100w_cout.wcs.to_header(), galaxy.upper(), os.path.join(work_dir,f\"{galaxy.upper()}_rgb.png\"), dpi=300, tickcolor='0.6', labelcolor='k', facecolor='w', minorticks=True)\n",
    "plt.imshow(img, origin='lower', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
